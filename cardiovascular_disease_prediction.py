# -*- coding: utf-8 -*-
"""Cardiovascular disease prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13joBfRHp1ic3a87kQbVnKvWEtstS-ryo

# **Importing Libraries and Read Dataset**
"""

import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

df = pd.read_csv('/content/drive/MyDrive/PRL/project/cardio_train.csv', delimiter = ';')

print(df.shape)
df.head()

df.info()

df.describe().T

df.nunique()

"""# **Cleaning the dataset**"""

from google.colab import drive
drive.mount('/content/drive')

"""**Removing unnecessary column**"""

df.drop('id', axis = 1, inplace = True)

"""**Checking duplicate rows**"""

df.duplicated().sum()

"""**Dropping the duplicate rows**"""

df.drop_duplicates(inplace= True)

"""**Checking null values for each column**"""

df.isnull().sum(axis = 0)

"""**Checking dataset for negative values**"""

count = (df < 0).sum().sum()
print(count)

"""**Taking only positive values**"""

df = df[(df['age']>0) & (df['gender']>-1) & (df['height']>-1) & (df['weight']>0) & 
        (df['ap_hi']>-1) & (df['ap_lo']>0) & (df['cholesterol']>0) & (df['gluc']>-1) & 
        (df['smoke']>-1) & (df['alco']>-1) & (df['active']>-1) & (df['cardio']>-1)]

print(df.shape)
df.head()

"""# **Dataset Visualization**"""

sns.set_style('whitegrid')
plt.figure(figsize=(15,11))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Dataset Correlation', loc='left', pad=20, fontsize=15)

def plot_stats(df, column, ax, color, angle):
  count_classes = df[column].value_counts()
  ax = sns.barplot(x=count_classes.index, y=count_classes, ax=ax, palette=color)
  ax.set_title(column.upper(), fontsize=18)
  for tick in ax.get_xticklabels():
    tick.set_rotation(angle)

"""**Age in the dataset**"""

plt.figure(figsize=(10, 7));
sns.histplot(df[['age']],bins=30,kde=True, color='b');
plt.title('age in days', loc='left', fontsize=15, pad=20);

"""**Gender**

---
1.   Female
2.   Male
"""

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,5))
fig.autofmt_xdate()
plot_stats(df, "gender", axes, "Blues", 0)
plt.show()

"""**Weight in the dataset**"""

plt.figure(figsize=(10, 7));
sns.histplot(df[['weight']],bins=30,kde=True, color='b');
plt.title('Weight in kg', loc='left', fontsize=15, pad=20);

"""**Height in the dataset**"""

plt.figure(figsize=(10, 7));
sns.histplot(df[['height']],bins=30,kde=True, color='red');
plt.title('Height in cm', loc='left', fontsize=15, pad=20);

"""**Cholesterol Level:**

---
1. Normal
2. Above normal
3. Well above normal
"""

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,5))
fig.autofmt_xdate()
plot_stats(df, "cholesterol", axes, "Blues", 0)
plt.show()

"""**Glucose Level:**

---
1. Normal
2. Above normal
3. Well above normal
"""

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,5))
fig.autofmt_xdate()
plot_stats(df, "gluc", axes, "Blues", 0)
plt.show()

"""**Somker or not**

---
0.   Non-somker
1.   Somker
"""

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,5))
fig.autofmt_xdate()
plot_stats(df, "smoke", axes, "Blues", 0)
plt.show()

"""**Drink alcohol or not**

---
0.   Non-alcoholic
1.   Alcoholic
"""

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,5))
fig.autofmt_xdate()
plot_stats(df, "alco", axes, "Blues", 0)
plt.show()

"""**Physically active or not**

---
0.   Active
1.   Not active
"""

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,5))
fig.autofmt_xdate()
plot_stats(df, "active", axes, "Blues", 0)
plt.show()

"""**Presence or absence of cardiovascular disease**

---
0. Absence
1. Presence
"""

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,5))
fig.autofmt_xdate()
plot_stats(df, "cardio", axes, "Blues", 0)
plt.show()

"""# **Feature Engineering**

**Rounding of the age in the dataset**
"""

df["age_year"] = round(df["age"] / 365)
print(df.shape)
df.head()

"""**Calculation of body mass index**"""

cols1 = df["weight"]
cols2 = df["height"] / 100
df["bmi"] = (cols1) / (cols2)**2

print(df.shape)
df.head()

"""**As we researched, Substracting Diastolic blood pressure from Systolic blood pressure gives Pulse Pressure that can't be negative
Therefore, ap_hi > ap_lo**
"""

print('There are total {} observations where ap_hi < ap_lo'.format(len(df[df['ap_hi'] < df['ap_lo']])))

df = df[df['ap_hi'] >= df['ap_lo']].reset_index(drop=True)
print(df.shape)
df.head()

"""# **Feature Selection**"""

sns.set_style('whitegrid')
plt.figure(figsize=(15,11))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Dataset Correlation', loc='left', pad=20, fontsize=15)

"""**Implement a Chi-Squared statistical test for non-negative features to select 5 of the best features from the dataset**"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
import numpy as np

X = df.drop(['cardio'], axis=1)
colname = list(X.columns)
X = np.array(X)
y = np.array(df['cardio'])

test = SelectKBest(score_func=chi2, k=5)
fit = test.fit(X, y)

feature_score = dict(zip(colname,fit.scores_))
sorted(feature_score.items(), key=lambda x:x[1])

"""# **Simple Model**"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

#X = df[['gender', 'ap_hi', 'ap_lo','cholesterol', 'gluc', 'age_year', 'bmi']]
#X = df[['gender', 'ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age_year', 'bmi']]
X1 = df[['ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age', 'bmi']]
y1 = df['cardio']
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=1)

scaler = StandardScaler()
X_train1 = scaler.fit_transform(X_train1)
X_test1 = scaler.transform(X_test1)

models1 = {"Logistic Regression": LogisticRegression(),
          "KNN" : KNeighborsClassifier(n_neighbors=50),
          "Decision tree" : DecisionTreeClassifier(),
          "Naive bayes" : GaussianNB(),
          "Linear SVM" : SVC(kernel='linear'),
          "Gaussian SVM" : SVC(kernel='rbf'),
          "Random forest" : RandomForestClassifier(n_estimators=100)}

accuracy1 = []
for i in models1.values():
  model = i
  model.fit(X_train1, y_train1)
  model_pred = model.predict(X_test1)
  accuracy1.append(accuracy_score(y_test1, model_pred))

for i,j in zip(models1, accuracy1):
  print(i,": ",j)

scores = dict(zip(list(models1.keys()), accuracy1))
scores_frame = pd.DataFrame(scores, index=["Accuracy Score"]).T
scores_frame.sort_values(by=["Accuracy Score"], axis=0 ,ascending=False, inplace=True)
scores_frame

plt.figure(figsize=(15,8))
sns.barplot(x=scores_frame.index, y=scores_frame["Accuracy Score"])
plt.xticks(rotation=0)

"""# **One hot Encoding**"""

newdataset2 = df[['ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age', 'bmi', 'cardio']]

#newdataset = df[['gender', 'ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age_year', 'bmi', 'cardio']]

#Gender
#genderdf = pd.get_dummies(newdataset['gender'])
#genderdf.columns = ['female','male']
#newdataset = newdataset.drop(['gender'], axis=1)

#cholesterol
coldf = pd.get_dummies(newdataset2['cholesterol'])
coldf.columns = ['normalCol', 'aboveNormalCol', 'wellAboveNormalCol']
newdataset2 = newdataset2.drop(['cholesterol'], axis=1)

#Glucose
glucdf = pd.get_dummies(newdataset2['gluc'])
glucdf.columns = ['normalgluc', 'aboveNormalgluc', 'wellAboveNormalgluc']
newdataset2 = newdataset2.drop(['gluc'], axis=1)


#newdataset = newdataset.join(genderdf)
newdataset2 = newdataset2.join(coldf)
newdataset2 = newdataset2.join(glucdf)
print(newdataset2.shape)
newdataset2.head()

X2 = newdataset2.drop(['cardio'], axis=1)
y2 = newdataset2['cardio']
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=1)

scaler = StandardScaler()
X_train2 = scaler.fit_transform(X_train2)
X_test2 = scaler.transform(X_test2)

models2 = {"Logistic Regression": LogisticRegression(),
          "KNN" : KNeighborsClassifier(n_neighbors=50),
          "Decision tree" : DecisionTreeClassifier(),
          "Naive bayes" : GaussianNB(),
          "Linear SVM" : SVC(kernel='linear'),
          "Gaussian SVM" : SVC(kernel='rbf'),
          "Random forest" : RandomForestClassifier(n_estimators=100)}

accuracy2 = []
for i in models2.values():
  model = i
  model.fit(X_train2, y_train2)
  model_pred = model.predict(X_test2)
  accuracy2.append(accuracy_score(y_test2, model_pred))

for i,j in zip(models2, accuracy2):
  print(i,": ",j)

scores = dict(zip(list(models2.keys()), accuracy2))
scores_frame = pd.DataFrame(scores, index=["Accuracy Score"]).T
scores_frame.sort_values(by=["Accuracy Score"], axis=0 ,ascending=False, inplace=True)
scores_frame

plt.figure(figsize=(15,8))
sns.barplot(x=scores_frame.index, y=scores_frame["Accuracy Score"])
plt.xticks(rotation=0)

"""# **Principal Component Analysis (PCA)**

**Mean centering**
"""

#X = df[['gender', 'ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age_year', 'bmi']]
X3 = df[['ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age', 'bmi']]
X3 = X3.subtract(X3.mean())
y3 = df['cardio']
X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.2, random_state=1)

scaler = StandardScaler()
X_train3 = scaler.fit_transform(X_train3)
X_test3 = scaler.transform(X_test3)

models3 = {"Logistic Regression": LogisticRegression(),
          "KNN" : KNeighborsClassifier(n_neighbors=50),
          "Decision tree" : DecisionTreeClassifier(),
          "Naive bayes" : GaussianNB(),
          "Linear SVM" : SVC(kernel='linear'),
          "Gaussian SVM" : SVC(kernel='rbf'),
          "Random forest" : RandomForestClassifier(n_estimators=100)}

from sklearn import decomposition
import numpy as np
import plotly.graph_objects as go

pca = decomposition.PCA()
pca.n_components = 2
pca_data = pca.fit_transform(X_train3)

pca_data = np.vstack((pca_data.T, y_train3)).T
pca_df = pd.DataFrame(data = pca_data, columns = ('first', 'second', 'label'))

fig = go.Figure(data=go.Scattergl(
    x = pca_df['first'], 
    y = pca_df['second'],
    mode='markers',
    marker_color=pca_df['label']
))

fig.show()

listmodels3 = ["Logistic Regression", "KNN", "Decision tree", "Naive bayes", "Linear SVM", "Gaussian SVM", "Random forest"]
accuracy3 = []
for i in range(2,9):
  pca = decomposition.PCA()
  pca.n_components = i
  pca_X_train = pca.fit_transform(X_train3)
  pca_X_test = pca.fit_transform(X_test3)

  j=0
  print("Dimension: ",pca_X_train.shape[1])
  for i in models3.values():
    model = i
    model.fit(pca_X_train, y_train3)
    model_pred = model.predict(pca_X_test)
    res = accuracy_score(y_test3, model_pred)
    accuracy3.append([listmodels3[j],pca_X_train.shape,res])

    print(listmodels3[j]," = ",res)
    j = j+1
  
  print("-----------------------------------------------------")

accuracy3.sort(key=lambda x: x[0])
accuracy3

"""# **OHE + PCA**"""

#newdataset = df[['gender', 'ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age_year', 'bmi', 'cardio']]
newdataset4 = df[['ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age', 'bmi', 'cardio']]
#Gender
#genderdf = pd.get_dummies(newdataset['gender'])
#genderdf.columns = ['female','male']
#newdataset = newdataset.drop(['gender'], axis=1)

#cholesterol
coldf = pd.get_dummies(newdataset4['cholesterol'])
coldf.columns = ['normalCol', 'aboveNormalCol', 'wellAboveNormalCol']
newdataset4 = newdataset4.drop(['cholesterol'], axis=1)

#cholesterol
glucdf = pd.get_dummies(newdataset4['gluc'])
glucdf.columns = ['normalgluc', 'aboveNormalgluc', 'wellAboveNormalgluc']
newdataset4 = newdataset4.drop(['gluc'], axis=1)


#newdataset = newdataset.join(genderdf)
newdataset4 = newdataset4.join(coldf)
newdataset4 = newdataset4.join(glucdf)
print(newdataset4.shape)
newdataset4.head()

X4 = newdataset4.drop(['cardio'], axis=1)
y4 = newdataset4['cardio']
X_train4, X_test4, y_train4, y_test4 = train_test_split(X4, y4, test_size=0.2, random_state=1)

scaler = StandardScaler()
X_train4 = scaler.fit_transform(X_train4)
X_test4 = scaler.transform(X_test4)

models4 = {"Logistic Regression": LogisticRegression(),
          "KNN" : KNeighborsClassifier(n_neighbors=50),
          "Decision tree" : DecisionTreeClassifier(),
          "Naive bayes" : GaussianNB(),
          "Linear SVM" : SVC(kernel='linear'),
          "Gaussian SVM" : SVC(kernel='rbf'),
          "Random forest" : RandomForestClassifier(n_estimators=100)}

listmodels4 = ["Logistic Regression", "KNN", "Decision tree", "Naive bayes", "Linear SVM", "Gaussian SVM", "Random forest"]
accuracy4 = []
for i in range(2,12):
  pca = decomposition.PCA()
  pca.n_components = i
  pca_X_train = pca.fit_transform(X_train4)
  pca_X_test = pca.fit_transform(X_test4)

  j=0
  print(pca_X_train.shape[1])
  for i in models4.values():
    model = i
    model.fit(pca_X_train, y_train4)
    model_pred = model.predict(pca_X_test)
    res = accuracy_score(y_test4, model_pred)
    accuracy4.append([listmodels4[j],pca_X_train.shape,res])

    print(listmodels4[j]," = ",res)
    j = j+1
  
  print("-----------------------------------------------------")

accuracy4.sort(key=lambda x: x[0])
accuracy4

"""# **Linear Discriminant Analysis (LDA)**

**Mean centering**
"""

#X = df[['gender', 'ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age_year', 'bmi']]
X5 = df[['ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age', 'bmi']]
X5 = X5.subtract(X5.mean())
y5 = df['cardio']
X_train5, X_test5, y_train5, y_test5 = train_test_split(X5, y5, test_size=0.2, random_state=1)

scaler = StandardScaler()
X_train5 = scaler.fit_transform(X_train5)
X_test5 = scaler.transform(X_test5)

models5 = {"Logistic Regression": LogisticRegression(),
          "KNN" : KNeighborsClassifier(n_neighbors=50),
          "Decision tree" : DecisionTreeClassifier(),
          "Naive bayes" : GaussianNB(),
          "Linear SVM" : SVC(kernel='linear'),
          "Gaussian SVM" : SVC(kernel='rbf'),
          "Random forest" : RandomForestClassifier(n_estimators=100)}

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(n_components=1)
lda_X_train = lda.fit_transform(X_train5, y_train5)
lda_X_test = lda.transform(X_test5)

accuracy5 = []
for i in models5.values():
  model = i
  model.fit(lda_X_train, y_train5)
  model_pred = model.predict(lda_X_test)
  accuracy5.append(accuracy_score(y_test5, model_pred))

for i,j in zip(models5, accuracy5):
  print(i,": ",j)

scores = dict(zip(list(models5.keys()), accuracy5))
scores_frame = pd.DataFrame(scores, index=["Accuracy Score"]).T
scores_frame.sort_values(by=["Accuracy Score"], axis=0 ,ascending=False, inplace=True)
scores_frame

plt.figure(figsize=(15,8))
sns.barplot(x=scores_frame.index, y=scores_frame["Accuracy Score"])
plt.xticks(rotation=0)

"""# **OHE + LDA**"""

#newdataset = df[['gender', 'ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age_year', 'bmi', 'cardio']]
newdataset6 = df[['ap_hi', 'ap_lo','cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age', 'bmi', 'cardio']]
#Gender
#genderdf = pd.get_dummies(newdataset['gender'])
#genderdf.columns = ['female','male']
#newdataset = newdataset.drop(['gender'], axis=1)

#cholesterol
coldf = pd.get_dummies(newdataset6['cholesterol'])
coldf.columns = ['normalCol', 'aboveNormalCol', 'wellAboveNormalCol']
newdataset6 = newdataset6.drop(['cholesterol'], axis=1)

#cholesterol
glucdf = pd.get_dummies(newdataset6['gluc'])
glucdf.columns = ['normalgluc', 'aboveNormalgluc', 'wellAboveNormalgluc']
newdataset6 = newdataset6.drop(['gluc'], axis=1)


#newdataset = newdataset.join(genderdf)
newdataset6 = newdataset6.join(coldf)
newdataset6 = newdataset6.join(glucdf)
print(newdataset6.shape)
newdataset6.head()

X6 = newdataset6.drop(['cardio'], axis=1)
y6 = newdataset6['cardio']
X_train6, X_test6, y_train6, y_test6 = train_test_split(X6, y6, test_size=0.2, random_state=1)

scaler = StandardScaler()
X_train6 = scaler.fit_transform(X_train6)
X_test6 = scaler.transform(X_test6)

models6 = {"Logistic Regression": LogisticRegression(),
          "KNN" : KNeighborsClassifier(n_neighbors=50),
          "Decision tree" : DecisionTreeClassifier(),
          "Naive bayes" : GaussianNB(),
          "Linear SVM" : SVC(kernel='linear'),
          "Gaussian SVM" : SVC(kernel='rbf'),
          "Random forest" : RandomForestClassifier(n_estimators=100)}

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(n_components=1)
lda_X_train = lda.fit_transform(X_train6, y_train6)
lda_X_test = lda.transform(X_test6)

accuracy6 = []
for i in models6.values():
  model = i
  model.fit(lda_X_train, y_train6)
  model_pred = model.predict(lda_X_test)
  accuracy6.append(accuracy_score(y_test6, model_pred))

for i,j in zip(models6, accuracy6):
  print(i,": ",j)

scores = dict(zip(list(models6.keys()), accuracy6))
scores_frame = pd.DataFrame(scores, index=["Accuracy Score"]).T
scores_frame.sort_values(by=["Accuracy Score"], axis=0 ,ascending=False, inplace=True)
scores_frame

plt.figure(figsize=(15,8))
sns.barplot(x=scores_frame.index, y=scores_frame["Accuracy Score"])
plt.xticks(rotation=0)

147